{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5a073f2-7380-40a1-b261-b634eeed18e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 03_silver_transform\n",
    "Purpose: Transform Bronze -> Silver using metadata-driven rules: type casting, deduplication, basic cleaning, and write Silver Delta tables.\n",
    "Author: Janak\n",
    "Date: 2025-11-26\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "085cb1a0-e701-4faf-8e9f-806a2fca032b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports & Paths"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, trim, when\n",
    "from pyspark.sql.types import *\n",
    "import json, os\n",
    "import datetime\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "bronze_base = \"/tmp/delta/bronze\"\n",
    "silver_base = \"/tmp/delta/silver\"\n",
    "metadata_path = \"src/metadata/metadata_schema.json\"\n",
    "\n",
    "# Optional: enable MLflow logging by setting this True and configuring MLflow experiment\n",
    "USE_MLFLOW = False\n",
    "if USE_MLFLOW:\n",
    "    import mlflow\n",
    "    mlflow.set_experiment(\"/Users/your.email@example.com/intelligent-etl-metadata-platform\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cb90a4b-7005-49dc-8a85-bd8ecf0dfb5f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Metadata"
    }
   },
   "outputs": [],
   "source": [
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(\"Tables in metadata:\", list(metadata.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50cc3527-ecd8-4297-994c-38c9774cf0ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper: cast types based on metadata (with safe fallbacks)"
    }
   },
   "outputs": [],
   "source": [
    "def cast_column_safe(df, col_name, dtype_str):\n",
    "    # map our simple type strings to Spark types (used by cast)\n",
    "    mapping = {\n",
    "        \"string\": \"string\",\n",
    "        \"timestamp\": \"timestamp\",\n",
    "        \"date\": \"date\",\n",
    "        \"double\": \"double\",\n",
    "        \"float\": \"float\",\n",
    "        \"int\": \"int\",\n",
    "        \"long\": \"long\",\n",
    "        \"boolean\": \"boolean\"\n",
    "    }\n",
    "    tgt = mapping.get(dtype_str.lower(), \"string\")\n",
    "    # avoid failing cast exceptions by using when/otherwise for common cases\n",
    "    # simple cast:\n",
    "    return df.withColumn(col_name, col(col_name).cast(tgt))\n",
    "    \n",
    "def apply_schema_casts(df, schema_dict):\n",
    "    for cname, dtype in schema_dict.items():\n",
    "        if cname in df.columns:\n",
    "            df = cast_column_safe(df, cname, dtype)\n",
    "        else:\n",
    "            # create null column with required name & type\n",
    "            df = df.withColumn(cname, col(lit(None)).cast(dtype if dtype in [\"string\",\"timestamp\",\"date\",\"double\",\"int\",\"float\",\"long\",\"boolean\"] else \"string\"))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be80db45-52c8-436b-9e8b-64602351fd50",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper: basic cleaning rules (trim strings, fill simple nulls)"
    }
   },
   "outputs": [],
   "source": [
    "def basic_cleaning(df, schema_dict):\n",
    "    # trim string columns, lowercase maybe for some columns if needed (not forced)\n",
    "    for cname, dtype in schema_dict.items():\n",
    "        if cname in df.columns and dtype.lower() == \"string\":\n",
    "            df = df.withColumn(cname, trim(col(cname)))\n",
    "    return df\n",
    "\n",
    "def deduplicate(df, pk_candidates):\n",
    "    # pk_candidates: list of columns to dedupe on; fallback to full row dedupe\n",
    "    if pk_candidates:\n",
    "        existing_cols = [c for c in pk_candidates if c in df.columns]\n",
    "        if existing_cols:\n",
    "            return df.dropDuplicates(existing_cols)\n",
    "    return df.dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3929494-d888-43c0-9cdd-68c82ccc0b7c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Process each table: cast, clean, dedupe, write Silver (main loop)"
    }
   },
   "outputs": [],
   "source": [
    "run_summary = []  # collect metrics for each table\n",
    "\n",
    "for table, meta in metadata.items():\n",
    "    print(f\"\\n=== Processing table: {table} ===\")\n",
    "    bronze_path = f\"{bronze_base}/{table}\"\n",
    "    silver_path = f\"{silver_base}/{table}\"\n",
    "    \n",
    "    # load bronze\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(bronze_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Skip {table}: cannot load bronze at {bronze_path} -> {e}\")\n",
    "        continue\n",
    "    \n",
    "    before_count = df.count()\n",
    "    print(f\"Bronze rows: {before_count}, columns: {len(df.columns)}\")\n",
    "    \n",
    "    # 1) Apply casts based on metadata.schema\n",
    "    schema_def = meta.get(\"schema\", {})\n",
    "    df_casted = apply_schema_casts(df, schema_def)\n",
    "    \n",
    "    # 2) Basic cleaning: trim strings, standardize date string->timestamp if needed\n",
    "    df_clean = basic_cleaning(df_casted, schema_def)\n",
    "    \n",
    "    # 3) Deduplicate: use primary key candidates if present in metadata (we use not_null as proxy)\n",
    "    pk_candidates = meta.get(\"quality_rules\", {}).get(\"not_null\", [])\n",
    "    df_dedup = deduplicate(df_clean, pk_candidates)\n",
    "    \n",
    "    # 4) Handle obvious issues: convert negative numeric to null for positive-only columns\n",
    "    pos_cols = meta.get(\"quality_rules\", {}).get(\"positive_values\", [])\n",
    "    for pc in pos_cols:\n",
    "        if pc in df_dedup.columns:\n",
    "            df_dedup = df_dedup.withColumn(pc, when(col(pc) < 0, None).otherwise(col(pc)))\n",
    "    \n",
    "    # 5) Partitioning hint: if table has a timestamp/date column called order_date or stock_date -> partition by date\n",
    "    partition_col = None\n",
    "    for cand in [\"order_date\", \"stock_date\", \"signup_date\", \"payment_date\"]:\n",
    "        if cand in df_dedup.columns:\n",
    "            # ensure date partition column exists as date (create if timestamp)\n",
    "            if dict(df_dedup.dtypes).get(cand, \"\").startswith(\"timestamp\"):\n",
    "                df_dedup = df_dedup.withColumn(cand + \"_date\", to_date(col(cand)))\n",
    "                partition_col = cand + \"_date\"\n",
    "            elif dict(df_dedup.dtypes).get(cand, \"\").startswith(\"date\"):\n",
    "                partition_col = cand\n",
    "            break\n",
    "    \n",
    "    # 6) Write Silver Delta\n",
    "    write_mode = \"overwrite\"\n",
    "    print(f\"Writing Silver to {silver_path} (partition: {partition_col})\")\n",
    "    if partition_col:\n",
    "        df_dedup.write.format(\"delta\").mode(write_mode).partitionBy(partition_col).save(silver_path)\n",
    "    else:\n",
    "        df_dedup.write.format(\"delta\").mode(write_mode).save(silver_path)\n",
    "    \n",
    "    after_count = spark.read.format(\"delta\").load(silver_path).count()\n",
    "    print(f\"Silver rows: {after_count}\")\n",
    "    \n",
    "    # register temp view for downstream notebooks\n",
    "    spark.read.format(\"delta\").load(silver_path).createOrReplaceTempView(f\"{table}_silver\")\n",
    "    print(f\"Registered temp view: {table}_silver\")\n",
    "    \n",
    "    # collect summary\n",
    "    run_summary.append({\n",
    "        \"table\": table,\n",
    "        \"bronze_rows\": before_count,\n",
    "        \"silver_rows\": after_count,\n",
    "        \"partition_col\": partition_col\n",
    "    })\n",
    "    \n",
    "    # Optional MLflow logging\n",
    "    if USE_MLFLOW:\n",
    "        mlflow.log_metric(f\"{table}_bronze_rows\", before_count)\n",
    "        mlflow.log_metric(f\"{table}_silver_rows\", after_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59427039-7591-4b1f-948b-e24f1e853199",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Summary DataFrame & Display"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "summary_rows = [Row(**s) for s in run_summary]\n",
    "if summary_rows:\n",
    "    summary_df = spark.createDataFrame(summary_rows)\n",
    "    display(summary_df)\n",
    "else:\n",
    "    print(\"No tables processed (check bronze paths).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b68d91b5-92e0-4d08-8661-21b74e41ba0c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Quick Data Quality Check (basic)"
    }
   },
   "outputs": [],
   "source": [
    "# Re-use metadata validation quick checks for Silver layer (nulls & enum mismatches)\n",
    "def quick_qc_on_silver(table, meta):\n",
    "    silver_path = f\"{silver_base}/{table}\"\n",
    "    df = spark.read.format(\"delta\").load(silver_path)\n",
    "    results = {}\n",
    "    rules = meta.get(\"quality_rules\", {})\n",
    "    if \"not_null\" in rules:\n",
    "        nr = {c: df.filter(df[c].isNull()).count() for c in rules[\"not_null\"] if c in df.columns}\n",
    "        results[\"not_null\"] = nr\n",
    "    if \"accepted_values\" in rules:\n",
    "        ev = {}\n",
    "        for col_name, allowed in rules[\"accepted_values\"].items():\n",
    "            if col_name in df.columns:\n",
    "                invalid = df.filter(~col(col_name).isin(allowed)).count()\n",
    "                ev[col_name] = invalid\n",
    "        results[\"accepted_values\"] = ev\n",
    "    return results\n",
    "\n",
    "qc_results = {}\n",
    "for table, meta in metadata.items():\n",
    "    try:\n",
    "        qc_results[table] = quick_qc_on_silver(table, meta)\n",
    "    except Exception as e:\n",
    "        qc_results[table] = {\"error\": str(e)}\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(qc_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8a552d6-f3de-462b-82ea-7dfe88bc8890",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save Silver Table Metadata (optional)"
    }
   },
   "outputs": [],
   "source": [
    "# Save a tiny lineage/manifest describing what we wrote (simple JSON inside repo)\n",
    "manifest = {\n",
    "    \"project\": \"intelligent-etl-metadata-platform\",\n",
    "    \"run_id\": datetime.datetime.now().isoformat(),\n",
    "    \"tables\": run_summary\n",
    "}\n",
    "manifest_path = \"/dbfs/tmp/intelligent_etl_manifest.json\"\n",
    "with open(manifest_path, \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print(\"Manifest written to:\", manifest_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_silver_transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
